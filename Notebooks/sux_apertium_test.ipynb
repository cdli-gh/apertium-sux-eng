{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraties\n",
    "import xml.etree.cElementTree as ET\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import subprocess\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sys\n",
    "import string\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# base path where we have all the bin files for sux-eng\n",
    "apertium_mt_path = '../'\n",
    "\n",
    "# NMT paths\n",
    "src_file_location = '../data/NMT_temp_data/src.txt'\n",
    "tgt_file_location = '../data/NMT_temp_data/tgt.txt'\n",
    "weight_location = '../../Data/_step_10000.pt'\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "chencherry = SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading t5 model for grammer error correction\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"Unbabel/gec-t5_small\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"SafiUllahShahid/EnGECmodel\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"SafiUllahShahid/EnGECmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_read(filename):\n",
    "    lines=[]\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_save(filename, sentence_list):\n",
    "    with open(filename, 'w') as filehandle:\n",
    "        for listitem in sentence_list:\n",
    "            filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run grammer error correction\n",
    "def grammer_error_correction(eng_sentences):\n",
    "    corrected_sentence = []\n",
    "    for eng_sentence in tqdm(eng_sentences):\n",
    "        tokenized_sentence = tokenizer(eng_sentence, max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        gec_result = tokenizer.decode(\n",
    "        model.generate(\n",
    "            input_ids = tokenized_sentence.input_ids,\n",
    "            attention_mask = tokenized_sentence.attention_mask, \n",
    "            max_length=512,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "        )[0],\n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        corrected_sentence.append(gec_result)\n",
    "\n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule based machine translation system\n",
    "# Note - there is no api call, here we are calling os.system('bash code') as there is no inbuilt python library for apertium \n",
    "def rule_machine_translation(sux_sentences):\n",
    "    sux_RBMT = []\n",
    "    for sux_sentence in sux_sentences:\n",
    "        try:\n",
    "            sux_sentence = sux_sentence.replace(':','-').replace('(','\\(').replace(')','\\)').replace(\"'\",\"\\\\'\").replace('|','\\|').replace('x','').replace('.','').replace('<','').replace('>','')\n",
    "            sux_sentence = \" \".join(sux_sentence.strip().split())\n",
    "            \n",
    "            ## Calling apertium rule based Engine\n",
    "            apertium_translation_command = f'''echo {sux_sentence} | apertium -d {apertium_mt_path} sux-eng'''\n",
    "            output = subprocess.check_output(apertium_translation_command, shell=True)\n",
    "\n",
    "            output = output.decode('ascii').strip().replace('x','').replace('#','').replace('-',' ').replace('*',' ').split()\n",
    "            output = \" \".join(output).lower()\n",
    "            sux_RBMT.append(output)\n",
    "        except:\n",
    "            sux_RBMT.append('')\n",
    "\n",
    "    return sux_RBMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural machine translation\n",
    "# Here also there is no python api call, rather it is a bash call using os.system\n",
    "def nn_machine_translation(sux_sentence):\n",
    "    txt_file_save(src_file_location, sux_sentence)\n",
    "\n",
    "    os.system(f'''onmt_translate -model {weight_location}  -src {src_file_location} -output {tgt_file_location}''')    \n",
    "    \n",
    "    sux_NNMT = txt_file_read(tgt_file_location)\n",
    "    \n",
    "    return sux_NNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_reference_preprocessing(eng_tranlation_reference):\n",
    "    eng_tranlation_reference = eng_tranlation_reference.replace('-',' ').lower()\n",
    "    eng_tranlation_reference = ''.join(i for i in s if i not in string.punctuation)\n",
    "    return eng_tranlation_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to call and process conll files \n",
    "# translating sentences from a directory containg conll files\n",
    "def process_conll_files(dir_path):\n",
    "    eng_reference_sentneces = []\n",
    "    sumerian_sentences = []\n",
    "    file_names = []\n",
    "\n",
    "    # extracting sumerian and english translation from the file\n",
    "    file_name = os.listdir(dir_path)\n",
    "\n",
    "    # reading data from conll files\n",
    "    for file in tqdm(file_name):\n",
    "\n",
    "        file_path = os.path.join(dir_path,file)\n",
    "        file_data = txt_file_read(file_path)\n",
    "\n",
    "        # extracting data from conll files \n",
    "        sux_sentence = ''\n",
    "        eng_tranlation_reference = ''\n",
    "        for row in file_data:\n",
    "            if row.startswith('# tr.en'):\n",
    "                eng_tranlation_reference = row.split('tr.en:')[1]\n",
    "            row_line = row.split('\\t')\n",
    "            if row_line[0].isdigit() and 'XPOSTAG' not in row:\n",
    "                sux_sentence+=row_line[1]+' '\n",
    "\n",
    "        # basic cleaning of english reference sentence so we do not miss correct words match because of basic errors like (Su-zen same as Suzen and suzen same as suzen)\n",
    "        eng_tranlation_reference = eng_reference_preprocessing(eng_tranlation_reference)\n",
    "\n",
    "        # basic cleaning of sumerian sentence before passing to rulebased translation\n",
    "        # sux_sentence = sux_sentence.replace('<','').replace('>','').lower()\n",
    "\n",
    "\n",
    "        eng_reference_sentneces.append(eng_tranlation_reference)\n",
    "        sumerian_sentences.append(sux_sentence)\n",
    "        file_names.append(file)\n",
    "\n",
    "\n",
    "    return file_names, eng_reference_sentneces, sumerian_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSLATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Translating conll files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '../data/mtaac_syntax_corpus_consolidated/dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:00<00:00, 11403.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract file names and english, sumerian sentences\n",
    "file_names, eng_tranlation_references, sux_sentences = process_conll_files(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Rule based machine translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Hard limit of 500 cohorts reached at cohort \"<szunigin>\" (#499) on line 0 - forcing break.\n",
      "Warning: Hard limit of 500 cohorts reached at cohort \"<2(asz)>\" (#499) on line 0 - forcing break.\n"
     ]
    }
   ],
   "source": [
    "sux_RBMT = rule_machine_translation(sux_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammer error correction if needed \n",
    "# gec_sux_RBMT = grammer_error_correction(sux_RBMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Neural machine translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 15:51:44,522 INFO] Translating shard 0.\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484612588/work/aten/src/ATen/native/Resize.cpp:24.)\n",
      "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:212: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self._batch_index = self.topk_ids // vocab_size\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [7, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484612588/work/aten/src/ATen/native/Resize.cpp:24.)\n",
      "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
      "[2022-09-12 15:52:09,891 INFO] PRED AVG SCORE: -0.3415, PRED PPL: 1.4070\n"
     ]
    }
   ],
   "source": [
    "sux_NNMT = nn_machine_translation(sux_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Calculating Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:00, 7115.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " average bleu score for rule based is (2.24364436822736, 2.026704983791064)\n",
      "\n",
      " average bleu score for neural network based is (8.060353282516942, 4.714877157589494)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#================================ BLEU SCORE ================================================ #\n",
    "translated_df = []\n",
    "rule_bleu_list = []\n",
    "nn_bleu_list = []\n",
    "\n",
    "\n",
    "for file, eng_tranlation_reference, sux_sentence,sux_RB,sux_NN in tqdm(zip(file_names,eng_tranlation_references,sux_sentences,sux_RBMT,sux_NNMT)):\n",
    "\n",
    "    rule_bleu = sentence_bleu([eng_tranlation_reference.split()], sux_RB.split(),smoothing_function=chencherry.method1,weights = (0.75,0.25,0,0))*100\n",
    "    rule_bleu_list.append(rule_bleu)\n",
    "\n",
    "\n",
    "    nn_bleu = sentence_bleu([eng_tranlation_reference.split()], sux_NN.split(), smoothing_function=chencherry.method1, weights = (0.75,0.25,0,0))*100\n",
    "    nn_bleu_list.append(nn_bleu)\n",
    "\n",
    "    translated_df.append([file, sux_sentence, eng_tranlation_reference, sux_RB, sux_NN, rule_bleu, nn_bleu])\n",
    "\n",
    "    # if count==33:\n",
    "    #     break\n",
    "\n",
    "\n",
    "print(f'''\\n average bleu score for rule based is''', (np.mean(rule_bleu_list),np.median(rule_bleu_list)))\n",
    "print(f'''\\n average bleu score for neural network based is''',(np.mean(nn_bleu_list),np.median(nn_bleu_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>sux_sentence</th>\n",
       "      <th>eng_tranlation_reference</th>\n",
       "      <th>eng_rule_based_translation</th>\n",
       "      <th>eng_nn_based_translation</th>\n",
       "      <th>rule_bleu</th>\n",
       "      <th>nn_bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P320163.conll</td>\n",
       "      <td>pisan-dub-ba mu 2(disz) sze-ba giri3-se3-ga ug...</td>\n",
       "      <td>Szusuen strong king king of Urim king of the f...</td>\n",
       "      <td>filing_basket one year ration attendant ugnim{...</td>\n",
       "      <td>Basketoftablets 2 years of the barley rations ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.198351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P125272.conll</td>\n",
       "      <td>usz2 ur-sila-luh 1(asz@c) GAN2 e2-ur2-bi-du10 ...</td>\n",
       "      <td>Szusuen strong king king of Urim king of the f...</td>\n",
       "      <td>dead &lt;n&gt; unit luh 1(towards his &lt;n&gt;@ c) field ...</td>\n",
       "      <td>the arasifieldwoods took in charge 1 acworker ...</td>\n",
       "      <td>0.190770</td>\n",
       "      <td>3.814166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P416458.conll</td>\n",
       "      <td>4(disz) ki szu-{d}idim-ta mu-kux(DU) iti ezem-...</td>\n",
       "      <td>Szusuen strong king king of Urim king of the f...</td>\n",
       "      <td>one day at hand { d} idim unknown year &lt;vble&gt; ...</td>\n",
       "      <td>4 garments from uIdim delivery of Ninazu year ...</td>\n",
       "      <td>1.315486</td>\n",
       "      <td>5.045325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P107372.conll</td>\n",
       "      <td>pisan-dub-ba kiszib3 didli masz-x-x e2 lu2-gi-...</td>\n",
       "      <td>Szusuen strong king king of Urim king of the f...</td>\n",
       "      <td>filing_basket several unit interest house brou...</td>\n",
       "      <td>Basketoftablets sealed documents varied from t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.785822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P101172.conll</td>\n",
       "      <td>5(disz) sila3 kasz saga 5(disz) sila3 ninda 5(...</td>\n",
       "      <td>Szusuen strong king king of Urim king of the f...</td>\n",
       "      <td>day month good beer day month bread day unit o...</td>\n",
       "      <td>for Inanna 5 sila3 fine beer 5 sila3 bread 5 s...</td>\n",
       "      <td>0.366923</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file                                       sux_sentence  \\\n",
       "0  P320163.conll  pisan-dub-ba mu 2(disz) sze-ba giri3-se3-ga ug...   \n",
       "1  P125272.conll  usz2 ur-sila-luh 1(asz@c) GAN2 e2-ur2-bi-du10 ...   \n",
       "2  P416458.conll  4(disz) ki szu-{d}idim-ta mu-kux(DU) iti ezem-...   \n",
       "3  P107372.conll  pisan-dub-ba kiszib3 didli masz-x-x e2 lu2-gi-...   \n",
       "4  P101172.conll  5(disz) sila3 kasz saga 5(disz) sila3 ninda 5(...   \n",
       "\n",
       "                            eng_tranlation_reference  \\\n",
       "0  Szusuen strong king king of Urim king of the f...   \n",
       "1  Szusuen strong king king of Urim king of the f...   \n",
       "2  Szusuen strong king king of Urim king of the f...   \n",
       "3  Szusuen strong king king of Urim king of the f...   \n",
       "4  Szusuen strong king king of Urim king of the f...   \n",
       "\n",
       "                          eng_rule_based_translation  \\\n",
       "0  filing_basket one year ration attendant ugnim{...   \n",
       "1  dead <n> unit luh 1(towards his <n>@ c) field ...   \n",
       "2  one day at hand { d} idim unknown year <vble> ...   \n",
       "3  filing_basket several unit interest house brou...   \n",
       "4  day month good beer day month bread day unit o...   \n",
       "\n",
       "                            eng_nn_based_translation  rule_bleu    nn_bleu  \n",
       "0  Basketoftablets 2 years of the barley rations ...   0.000000  13.198351  \n",
       "1  the arasifieldwoods took in charge 1 acworker ...   0.190770   3.814166  \n",
       "2  4 garments from uIdim delivery of Ninazu year ...   1.315486   5.045325  \n",
       "3  Basketoftablets sealed documents varied from t...   0.000000   8.785822  \n",
       "4  for Inanna 5 sila3 fine beer 5 sila3 bread 5 s...   0.366923   0.000000  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for test and analysis\n",
    "col_name = ['file', 'sux_sentence', 'eng_tranlation_reference', 'eng_rule_based_translation', 'eng_nn_based_translation', 'rule_bleu', 'nn_bleu']\n",
    "trainslation_pd = pd.DataFrame(translated_df,columns = col_name)\n",
    "trainslation_pd.to_csv('../results/translation_results.csv')\n",
    "trainslation_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Translating sumerian txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt file location\n",
    "txt_file_location = '../test/sux-eng-input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e2 lugal-la mu-un-du3', 'lugal', 'dummu']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sux_sentences_list = txt_file_read(txt_file_location)\n",
    "sux_sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house intercalary_month <np> year un quality_designation',\n",
       " 'intercalary_month',\n",
       " 'dummu']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a. rule based engine\n",
    "RBMT_result = rule_machine_translation(sux_sentences_list)\n",
    "RBMT_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 15:52:11,751 INFO] Translating shard 0.\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [15], which does not match the required output shape [3, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484612588/work/aten/src/ATen/native/Resize.cpp:24.)\n",
      "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:212: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self._batch_index = self.topk_ids // vocab_size\n",
      "[2022-09-12 15:52:11,935 INFO] PRED AVG SCORE: -0.7102, PRED PPL: 2.0343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['built the temple of the king he built', 'king king', 'Tirmium and']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. NMT engine\n",
    "NMT_results = nn_machine_translation(sux_sentences_list)\n",
    "NMT_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translating single sumerian sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "sux_sentence = \"{d}szul-gi nita kal-ga lugal uri5{ki}-ma lugal an ub-da limmu2-ba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['szulgir male month intercalary_month year intercalary_month an <n> king']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a. rule based engine\n",
    "RBMT_result = rule_machine_translation([sux_sentence])\n",
    "# final_sentence = grammer_error_correction(RBMT_result)\n",
    "RBMT_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-12 15:52:13,568 INFO] Translating shard 0.\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484612588/work/aten/src/ATen/native/Resize.cpp:24.)\n",
      "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:212: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self._batch_index = self.topk_ids // vocab_size\n",
      "[2022-09-12 15:52:13,792 INFO] PRED AVG SCORE: -0.0810, PRED PPL: 1.0844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ulgi mighty man king of Ur and king of the four world quarters']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. NMT engine\n",
    "NNMT_results = nn_machine_translation([sux_sentence])\n",
    "NNMT_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " average bleu score for rule based is 5.152710423703126\n",
      "\n",
      " average bleu score for neural network based is 47.76235168541087\n"
     ]
    }
   ],
   "source": [
    "# c. calculate bleu score \n",
    "\n",
    "# pass the reference senetnce \n",
    "eng_tranlation_reference = \"Szusuen, strong king, king of Urim, king of the four quarters\"\n",
    "eng_tranlation_reference = eng_reference_preprocessing(eng_tranlation_reference)\n",
    "\n",
    "rule_bleu = sentence_bleu([eng_tranlation_reference.split()], RBMT_result[0].split(),smoothing_function=chencherry.method1,weights = (0.75,0.25,0,0))*100\n",
    "nn_bleu = sentence_bleu([eng_tranlation_reference.split()], NNMT_results[0].split(), smoothing_function=chencherry.method1, weights = (0.75,0.25,0,0))*100\n",
    "\n",
    "print(f'''\\n average bleu score for rule based is''', rule_bleu)\n",
    "print(f'''\\n average bleu score for neural network based is''',nn_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exrta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK BlEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference = [['this','boy']]\n",
    "candidate = ['this', 'is']\n",
    "chencherry = SmoothingFunction()\n",
    "score = sentence_bleu(reference, candidate, smoothing_function=chencherry.method1, weights=(1,0,0,0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bleu([eng_tranlation_reference.split()], eng_nn_based_translation.split(), smoothing_function=chencherry.method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0, 'counts': [2, 1, 0, 0], 'totals': [2, 1, 0, 0], 'precisions': [100.0, 100.0, 0.0, 0.0], 'bp': 1.0, 'sys_len': 2, 'ref_len': 2}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"hello there\"]\n",
    "references = [\n",
    "     [\"hello bro\"],\n",
    " ]\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references, smooth_method= 'floor')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"hello\"]\n",
    "references = [[\"hello there\"]]\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "results = sacrebleu.compute(predictions=predictions, \n",
    "                             references=references, smooth_method = 'add-k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 36.78794411714425,\n",
       " 'counts': [1, 1, 1, 1],\n",
       " 'totals': [1, 1, 1, 1],\n",
       " 'precisions': [100.0, 100.0, 100.0, 100.0],\n",
       " 'bp': 0.36787944117144233,\n",
       " 'sys_len': 1,\n",
       " 'ref_len': 2}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3244.34814453125\n",
      "199 2151.0498046875\n",
      "299 1427.306884765625\n",
      "399 948.1649780273438\n",
      "499 630.9312744140625\n",
      "599 420.8768310546875\n",
      "699 281.77752685546875\n",
      "799 189.65609741210938\n",
      "899 128.6400909423828\n",
      "999 88.22222900390625\n",
      "1099 61.44561004638672\n",
      "1199 43.70391082763672\n",
      "1299 31.947072982788086\n",
      "1399 24.154930114746094\n",
      "1499 18.989810943603516\n",
      "1599 15.565485000610352\n",
      "1699 13.2947998046875\n",
      "1799 11.788824081420898\n",
      "1899 10.789817810058594\n",
      "1999 10.126983642578125\n",
      "Result: y = -0.009623829275369644 + 0.8226878643035889 x + 0.0016602700343355536 x^2 + -0.08848666399717331 x^3\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "# Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('gsoc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b2059aca115e59cd7c84b0223fc0dde900163694b67bbfbf2e88c019072cf6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
