{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing libraties\n",
    "import xml.etree.cElementTree as ET\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import subprocess\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sys\n",
    "import string\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# base path where we have all the bin files for sux-eng\n",
    "apertium_mt_path = '../'\n",
    "\n",
    "# NMT paths\n",
    "src_file_location = '../data/NMT_temp_data/src.txt'\n",
    "tgt_file_location = '../data/NMT_temp_data/tgt.txt'\n",
    "weight_location = '../../Data/_step_10000.pt'\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "chencherry = SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading t5 model for grammer error correction\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"Unbabel/gec-t5_small\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"SafiUllahShahid/EnGECmodel\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"SafiUllahShahid/EnGECmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run grammer error correction\n",
    "# def grammer_error_correction(eng_sentences):\n",
    "#     corrected_sentence = []\n",
    "#     for eng_sentence in tqdm(eng_sentences):\n",
    "#         tokenized_sentence = tokenizer(eng_sentence, max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n",
    "#         gec_result = tokenizer.decode(\n",
    "#         model.generate(\n",
    "#             input_ids = tokenized_sentence.input_ids,\n",
    "#             attention_mask = tokenized_sentence.attention_mask, \n",
    "#             max_length=512,\n",
    "#             num_beams=5,\n",
    "#             early_stopping=True,\n",
    "#         )[0],\n",
    "#         skip_special_tokens=True, \n",
    "#         clean_up_tokenization_spaces=True\n",
    "#         )\n",
    "#         corrected_sentence.append(gec_result)\n",
    "\n",
    "#     return corrected_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_read(filename):\n",
    "    lines=[]\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_file_save(filename, sentence_list):\n",
    "    with open(filename, 'w') as filehandle:\n",
    "        for listitem in sentence_list:\n",
    "            filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english translation replacer\n",
    "# we need to replace this so that it can be compiled with lexd apertium analyzer\n",
    "def eng_word_processing(engword):\n",
    "    word = engword.replace('â€™','').replace('\\'','')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumerian form replacer\n",
    "# we need to replace this so that it can be compiled with lexd analyzer as well as for bilingual dictionary\n",
    "def sux_word_processing(suxword):\n",
    "    processed_word = ''\n",
    "    for i,char in enumerate(suxword):\n",
    "        if char in string.punctuation:\n",
    "            processed_word+='\\\\'+char\n",
    "        else:\n",
    "            processed_word+=char\n",
    "    # word = suxword.replace('{','\\{').replace('}','\\}').replace('[','\\[').replace(']','\\]').replace('(','\\(').replace(')','\\)').replace('|','\\|')\\\n",
    "    #     .replace('<','').replace('>','').replace('@','').replace('.','').replace(':','-').replace('x','') # to escape special meaning of brackets and \n",
    "    return processed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule based machine translation system\n",
    "# Note - there is no api call, here we are calling os.system('bash code') as there is no inbuilt python library for apertium \n",
    "def rule_machine_translation(sux_sentences):\n",
    "    sux_RBMT = []\n",
    "    for sux_sentence in sux_sentences:\n",
    "        try:\n",
    "            sux_sentence = sux_word_processing(sux_sentence)\n",
    "            # sux_sentence = \" \".join(sux_sentence.strip().split())\n",
    "            \n",
    "            ## Calling apertium rule based Engine\n",
    "            apertium_translation_command = f'''echo {sux_sentence} | apertium -d {apertium_mt_path} sux-eng'''\n",
    "            output = subprocess.check_output(apertium_translation_command, shell=True)\n",
    "            output = output.decode('ascii').strip().replace('x','').replace('#','').replace('*',' ').split()\n",
    "            output = \" \".join(output)\n",
    "            sux_RBMT.append(output)\n",
    "        except:\n",
    "            sux_RBMT.append('')\n",
    "\n",
    "    return sux_RBMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural machine translation\n",
    "# Here also there is no python api call, rather it is a bash call using os.system\n",
    "def nn_machine_translation(sux_sentence):\n",
    "    txt_file_save(src_file_location, sux_sentence)\n",
    "\n",
    "    os.system(f'''onmt_translate -model {weight_location}  -src {src_file_location} -output {tgt_file_location}''')    \n",
    "    \n",
    "    sux_NNMT = txt_file_read(tgt_file_location)\n",
    "    \n",
    "    return sux_NNMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eng_reference_preprocessing(eng_tranlation_reference):\n",
    "#     eng_tranlation_reference = eng_tranlation_reference.replace('-',' ').lower()\n",
    "#     eng_tranlation_reference = ''.join(i for i in s if i not in string.punctuation)\n",
    "#     return eng_tranlation_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to call and process conll files \n",
    "# translating sentences from a directory containg conll files\n",
    "def process_conll_files(dir_path):\n",
    "    eng_reference_sentneces = []\n",
    "    sumerian_sentences = []\n",
    "    file_names = []\n",
    "\n",
    "    # extracting sumerian and english translation from the file\n",
    "    file_name = os.listdir(dir_path)\n",
    "\n",
    "    # reading data from conll files\n",
    "    for file in tqdm(file_name):\n",
    "\n",
    "        file_path = os.path.join(dir_path,file)\n",
    "        file_data = txt_file_read(file_path)\n",
    "\n",
    "        # extracting data from conll files \n",
    "        sux_sentence = ''\n",
    "        eng_tranlation_reference = ''\n",
    "        for row in file_data:\n",
    "            if row.startswith('# tr.en'):\n",
    "                eng_tranlation_reference = row.split('tr.en:')[1]\n",
    "            row_line = row.split('\\t')\n",
    "            if row_line[0].isdigit() and 'XPOSTAG' not in row:\n",
    "                sux_sentence+=row_line[1]+' '\n",
    "\n",
    "        # basic cleaning of english reference sentence so we do not miss correct words match because of basic errors like (Su-zen same as Suzen and suzen same as suzen)\n",
    "        eng_tranlation_reference = eng_word_processing(eng_tranlation_reference)\n",
    "\n",
    "        # basic cleaning of sumerian sentence before passing to rulebased translation\n",
    "        # sux_sentence = sux_sentence.replace('<','').replace('>','').lower()\n",
    "\n",
    "\n",
    "        eng_reference_sentneces.append(eng_tranlation_reference)\n",
    "        sumerian_sentences.append(sux_sentence)\n",
    "        file_names.append(file)\n",
    "\n",
    "\n",
    "    return file_names, eng_reference_sentneces, sumerian_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSLATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Translating conll files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '../data/mtaac_syntax_corpus_consolidated/dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:00<00:00, 7849.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract file names and english, sumerian sentences\n",
    "file_names, eng_tranlation_references, sux_sentences = process_conll_files(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Rule based machine translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Hard limit of 500 cohorts reached at cohort \"<gurum2>\" (#499) on line 0 - forcing break.\n",
      "libc++abi: terminating with uncaught exception of type std::invalid_argument: unable to parse int\n",
      "Warning: Hard limit of 500 cohorts reached at cohort \"<5(disz)>\" (#499) on line 0 - forcing break.\n"
     ]
    }
   ],
   "source": [
    "sux_RBMT = rule_machine_translation(sux_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammer error correction if needed \n",
    "# gec_sux_RBMT = grammer_error_correction(sux_RBMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Neural machine translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-17 12:46:22,422 INFO] Translating shard 0.\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [150], which does not match the required output shape [30, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484612588/work/aten/src/ATen/native/Resize.cpp:24.)\n",
      "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:212: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self._batch_index = self.topk_ids // vocab_size\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [35], which does not match the required output shape [7, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484612588/work/aten/src/ATen/native/Resize.cpp:24.)\n",
      "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
      "[2022-09-17 12:46:59,196 INFO] PRED AVG SCORE: -0.3415, PRED PPL: 1.4070\n"
     ]
    }
   ],
   "source": [
    "sux_NNMT = nn_machine_translation(sux_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Calculating Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:00, 3787.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " average bleu score for rule based is (19.156694424387524, 20.451732834048002)\n",
      "\n",
      " average bleu score for neural network based is (18.86855003949193, 6.881713720035471)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#================================ BLEU SCORE ================================================ #\n",
    "translated_df = []\n",
    "rule_bleu_list = []\n",
    "nn_bleu_list = []\n",
    "\n",
    "\n",
    "for file, eng_tranlation_reference, sux_sentence,sux_RB,sux_NN in tqdm(zip(file_names,eng_tranlation_references,sux_sentences,sux_RBMT,sux_NNMT)):\n",
    "\n",
    "    rule_bleu = sentence_bleu([eng_tranlation_reference.split()], sux_RB.split(),smoothing_function=chencherry.method1,weights = (0.75,0.25,0,0))*100\n",
    "    rule_bleu_list.append(rule_bleu)\n",
    "\n",
    "\n",
    "    nn_bleu = sentence_bleu([eng_tranlation_reference.split()], sux_NN.split(), smoothing_function=chencherry.method1, weights = (0.75,0.25,0,0))*100\n",
    "    nn_bleu_list.append(nn_bleu)\n",
    "\n",
    "    translated_df.append([file, sux_sentence, eng_tranlation_reference, sux_RB, sux_NN, rule_bleu, nn_bleu])\n",
    "\n",
    "    # if count==33:\n",
    "    #     break\n",
    "\n",
    "\n",
    "print(f'''\\n average bleu score for rule based is''', (np.mean(rule_bleu_list),np.median(rule_bleu_list)))\n",
    "print(f'''\\n average bleu score for neural network based is''',(np.mean(nn_bleu_list),np.median(nn_bleu_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>sux_sentence</th>\n",
       "      <th>eng_tranlation_reference</th>\n",
       "      <th>eng_rule_based_translation</th>\n",
       "      <th>eng_nn_based_translation</th>\n",
       "      <th>rule_bleu</th>\n",
       "      <th>nn_bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P320163.conll</td>\n",
       "      <td>pisan-dub-ba mu 2(disz) sze-ba giri3-se3-ga ug...</td>\n",
       "      <td>Basket-of-tablets years of rations personnel ...</td>\n",
       "      <td>Basket-of-tablets one year ration attendant Ug...</td>\n",
       "      <td>Basketoftablets 2 years of the barley rations ...</td>\n",
       "      <td>11.908330</td>\n",
       "      <td>4.776235e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P125272.conll</td>\n",
       "      <td>usz2 ur-sila-luh 1(asz@c) GAN2 e2-ur2-bi-du10 ...</td>\n",
       "      <td>Dead Ur-silalu a-c- field E-urbidu a foreman ...</td>\n",
       "      <td>dead Ur-silaluh 1(towards his Na-silim@ c) fie...</td>\n",
       "      <td>the arasifieldwoods took in charge 1 acworker ...</td>\n",
       "      <td>17.613294</td>\n",
       "      <td>1.542663e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P416458.conll</td>\n",
       "      <td>4(disz) ki szu-{d}idim-ta mu-kux(DU) iti ezem-...</td>\n",
       "      <td>oxen cows male equids female equids old from ...</td>\n",
       "      <td>one from Szu-Idim delivery month in Ezemninazu...</td>\n",
       "      <td>4 garments from uIdim delivery of Ninazu year ...</td>\n",
       "      <td>28.661937</td>\n",
       "      <td>4.361240e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P107372.conll</td>\n",
       "      <td>pisan-dub-ba kiszib3 didli masz-x-x e2 lu2-gi-...</td>\n",
       "      <td>Basket-of-tablets sealed documents varied fro...</td>\n",
       "      <td>Basket-of-tablets several seal documents house...</td>\n",
       "      <td>Basketoftablets sealed documents varied from t...</td>\n",
       "      <td>17.467769</td>\n",
       "      <td>6.916163e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P101172.conll</td>\n",
       "      <td>5(disz) sila3 kasz saga 5(disz) sila3 ninda 5(...</td>\n",
       "      <td>sila fine beer sila bread shekels onions shek...</td>\n",
       "      <td>one unit good beer one unit one bread unit one...</td>\n",
       "      <td>for Inanna 5 sila3 fine beer 5 sila3 bread 5 s...</td>\n",
       "      <td>14.528770</td>\n",
       "      <td>5.314962e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file                                       sux_sentence  \\\n",
       "0  P320163.conll  pisan-dub-ba mu 2(disz) sze-ba giri3-se3-ga ug...   \n",
       "1  P125272.conll  usz2 ur-sila-luh 1(asz@c) GAN2 e2-ur2-bi-du10 ...   \n",
       "2  P416458.conll  4(disz) ki szu-{d}idim-ta mu-kux(DU) iti ezem-...   \n",
       "3  P107372.conll  pisan-dub-ba kiszib3 didli masz-x-x e2 lu2-gi-...   \n",
       "4  P101172.conll  5(disz) sila3 kasz saga 5(disz) sila3 ninda 5(...   \n",
       "\n",
       "                            eng_tranlation_reference  \\\n",
       "0   Basket-of-tablets years of rations personnel ...   \n",
       "1   Dead Ur-silalu a-c- field E-urbidu a foreman ...   \n",
       "2   oxen cows male equids female equids old from ...   \n",
       "3   Basket-of-tablets sealed documents varied fro...   \n",
       "4   sila fine beer sila bread shekels onions shek...   \n",
       "\n",
       "                          eng_rule_based_translation  \\\n",
       "0  Basket-of-tablets one year ration attendant Ug...   \n",
       "1  dead Ur-silaluh 1(towards his Na-silim@ c) fie...   \n",
       "2  one from Szu-Idim delivery month in Ezemninazu...   \n",
       "3  Basket-of-tablets several seal documents house...   \n",
       "4  one unit good beer one unit one bread unit one...   \n",
       "\n",
       "                            eng_nn_based_translation  rule_bleu       nn_bleu  \n",
       "0  Basketoftablets 2 years of the barley rations ...  11.908330  4.776235e+01  \n",
       "1  the arasifieldwoods took in charge 1 acworker ...  17.613294  1.542663e-15  \n",
       "2  4 garments from uIdim delivery of Ninazu year ...  28.661937  4.361240e+01  \n",
       "3  Basketoftablets sealed documents varied from t...  17.467769  6.916163e+01  \n",
       "4  for Inanna 5 sila3 fine beer 5 sila3 bread 5 s...  14.528770  5.314962e-03  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for test and analysis\n",
    "col_name = ['file', 'sux_sentence', 'eng_tranlation_reference', 'eng_rule_based_translation', 'eng_nn_based_translation', 'rule_bleu', 'nn_bleu']\n",
    "trainslation_pd = pd.DataFrame(translated_df,columns = col_name)\n",
    "trainslation_pd.to_csv('../results/translation_mtaac_dev_results.csv')\n",
    "trainslation_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Translating sumerian txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt file location\n",
    "txt_file_location = '../test/sux-eng-input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pisan-dub-ba nig2-ka9-ak ab-ba-mu giri3 a-tu']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sux_sentences_list = txt_file_read(txt_file_location)\n",
    "sux_sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Basket-of-tablets account of Abbamu and under the authority of Atu']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a. rule based engine\n",
    "# cat [file] | apertium -d . sux-eng\n",
    "RBMT_result = rule_machine_translation(sux_sentences_list)\n",
    "RBMT_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-17 20:04:20,859 INFO] Translating shard 0.\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484612588/work/aten/src/ATen/native/Resize.cpp:24.)\n",
      "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:212: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self._batch_index = self.topk_ids // vocab_size\n",
      "[2022-09-17 20:04:21,037 INFO] PRED AVG SCORE: -0.1070, PRED PPL: 1.1129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Basketoftablets accounts of Abbamu via Atu']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. NMT engine\n",
    "NMT_results = nn_machine_translation(sux_sentences_list)\n",
    "NMT_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translating single sumerian sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sux_sentence = \"mu {d}szu-{d}suen lugal uri5{ki}-ma-ke4 ma2-dara3-abzu {d}en-ki-ka bi2-in-du8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year Szusuen king of Urim caulked Madaraabzu of Enkik']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a. rule based engine\n",
    "RBMT_result = rule_machine_translation([sux_sentence])\n",
    "# final_sentence = grammer_error_correction(RBMT_result)\n",
    "RBMT_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-17 20:09:42,692 INFO] Translating shard 0.\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:209: UserWarning: An output with one or more elements was resized since it had shape [5], which does not match the required output shape [1, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1659484612588/work/aten/src/ATen/native/Resize.cpp:24.)\n",
      "  torch.mul(self.topk_scores, length_penalty, out=self.topk_log_probs)\n",
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/onmt/translate/beam_search.py:212: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self._batch_index = self.topk_ids // vocab_size\n",
      "[2022-09-17 20:09:43,020 INFO] PRED AVG SCORE: -0.1292, PRED PPL: 1.1379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['year By uSuen king of Ur the boat IbexofApsu of Enki was caulked']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. NMT engine\n",
    "NNMT_results = nn_machine_translation([sux_sentence])\n",
    "NNMT_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " average bleu score for rule based is 14.163412623658234\n",
      "\n",
      " average bleu score for neural network based is 16.900621985565852\n"
     ]
    }
   ],
   "source": [
    "# c. calculate bleu score \n",
    "\n",
    "# pass the reference senetnce \n",
    "eng_tranlation_reference = \"in the year when Szusuen, king of Urim, caulked the Madaraabzu of Enki\"\n",
    "eng_tranlation_reference = eng_word_processing(eng_tranlation_reference)\n",
    "\n",
    "rule_bleu = sentence_bleu([eng_tranlation_reference.split()], RBMT_result[0].split(),smoothing_function=chencherry.method1,weights = (0.75,0.25,0,0))*100\n",
    "nn_bleu = sentence_bleu([eng_tranlation_reference.split()], NNMT_results[0].split(), smoothing_function=chencherry.method1, weights = (0.75,0.25,0,0))*100\n",
    "\n",
    "print(f'''\\n average bleu score for rule based is''', rule_bleu)\n",
    "print(f'''\\n average bleu score for neural network based is''',nn_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exrta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK BlEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference = [['this','boy']]\n",
    "candidate = ['this', 'is']\n",
    "chencherry = SmoothingFunction()\n",
    "score = sentence_bleu(reference, candidate, smoothing_function=chencherry.method1, weights=(1,0,0,0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bleu([eng_tranlation_reference.split()], eng_nn_based_translation.split(), smoothing_function=chencherry.method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/gsoc/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0, 'counts': [2, 1, 0, 0], 'totals': [2, 1, 0, 0], 'precisions': [100.0, 100.0, 0.0, 0.0], 'bp': 1.0, 'sys_len': 2, 'ref_len': 2}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"hello there\"]\n",
    "references = [\n",
    "     [\"hello bro\"],\n",
    " ]\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references, smooth_method= 'floor')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"hello\"]\n",
    "references = [[\"hello there\"]]\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "results = sacrebleu.compute(predictions=predictions, \n",
    "                             references=references, smooth_method = 'add-k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 36.78794411714425,\n",
       " 'counts': [1, 1, 1, 1],\n",
       " 'totals': [1, 1, 1, 1],\n",
       " 'precisions': [100.0, 100.0, 100.0, 100.0],\n",
       " 'bp': 0.36787944117144233,\n",
       " 'sys_len': 1,\n",
       " 'ref_len': 2}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3244.34814453125\n",
      "199 2151.0498046875\n",
      "299 1427.306884765625\n",
      "399 948.1649780273438\n",
      "499 630.9312744140625\n",
      "599 420.8768310546875\n",
      "699 281.77752685546875\n",
      "799 189.65609741210938\n",
      "899 128.6400909423828\n",
      "999 88.22222900390625\n",
      "1099 61.44561004638672\n",
      "1199 43.70391082763672\n",
      "1299 31.947072982788086\n",
      "1399 24.154930114746094\n",
      "1499 18.989810943603516\n",
      "1599 15.565485000610352\n",
      "1699 13.2947998046875\n",
      "1799 11.788824081420898\n",
      "1899 10.789817810058594\n",
      "1999 10.126983642578125\n",
      "Result: y = -0.009623829275369644 + 0.8226878643035889 x + 0.0016602700343355536 x^2 + -0.08848666399717331 x^3\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "# Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('gsoc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b2059aca115e59cd7c84b0223fc0dde900163694b67bbfbf2e88c019072cf6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
